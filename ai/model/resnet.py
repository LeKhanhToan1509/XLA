"""
ResNet (Residual Network) Implementation
======================================

C√¥ng th·ª©c ch√≠nh:
- Residual connection: F(x) + x = H(x)
  Trong ƒë√≥:
  - F(x): H·ªçc residual mapping
  - x: Identity shortcut connection
  - H(x): Desired underlying mapping

ƒê·∫ßu v√†o:
- Input tensor shape: (batch_size, channels, height, width)
- Labels shape: (batch_size,) ho·∫∑c (batch_size, num_classes) cho one-hot

Ki·∫øn tr√∫c:
1. Conv layer ƒë·∫ßu ti√™n: 7x7, 64 filters
2. Max pooling: 3x3
3. 4 nh√≥m residual blocks v·ªõi s·ªë filters tƒÉng d·∫ßn: 64->128->256->512
4. Global average pooling
5. Fully connected layer cho classification
"""

try:
    import cupy as np
    print("‚úÖ Using GPU (CuPy)")
    GPU_AVAILABLE = True
except ImportError:
    import numpy as np
    print("‚ö†Ô∏è  Using CPU (NumPy)")
    GPU_AVAILABLE = False
from ai.model.layers import Conv, BatchNorm, ReLU, MaxPool, Fc, Softmax, CrossEntropyLoss
from ai.utils.optimizers import Adam
from ai.configs.config import NUM_CLASSES, INPUT_SHAPE

class ResidualBlock:
    """
    Residual Block Implementation
    ============================
    
    C√¥ng th·ª©c Residual Block:
    y = F(x, {Wi}) + x
    
    Trong ƒë√≥:
    - F(x, {Wi}): Stack c·ªßa c√°c layers (Conv -> BN -> ReLU -> Conv -> BN)
    - x: Identity shortcut (c√≥ th·ªÉ c√≥ downsample n·∫øu dimensions kh√°c nhau)
    
    ƒê·∫ßu v√†o:
    - x: Feature maps v·ªõi shape (batch_size, in_channels, height, width)
    
    ƒê·∫ßu ra:
    - y: Feature maps v·ªõi shape (batch_size, out_channels, height/stride, width/stride)
    """
    def __init__(self, in_channels, out_channels, stride=1, downsample=False):
        self.conv1 = Conv(out_channels, 1, in_channels, stride=stride, padding=0)
        self.bn1 = BatchNorm(out_channels)
        self.relu1 = ReLU()
        self.conv2 = Conv(out_channels, 3, out_channels, stride=1, padding=1)
        self.bn2 = BatchNorm(out_channels)
        self.relu2 = ReLU()
        self.downsample = None
        self.bn_down = None
        if downsample or in_channels != out_channels:
            self.downsample = Conv(out_channels, 1, in_channels, stride=stride, padding=0)
            self.bn_down = BatchNorm(out_channels)
        self.relu_final = ReLU()
        self.layers = [self.conv1, self.bn1, self.relu1, self.conv2, self.bn2, self.relu2, self.relu_final]
        if self.downsample:
            self.layers += [self.downsample, self.bn_down]

    def forward(self, X, is_inference=False):
        """
        Forward pass c·ªßa Residual Block
        
        C√¥ng th·ª©c th·ª±c hi·ªán:
        1. F(x) = ReLU(BN(Conv(ReLU(BN(Conv(x))))))
        2. identity = x ho·∫∑c downsample(x) n·∫øu c·∫ßn
        3. output = F(x) + identity
        4. output = ReLU(output)
        
        ƒê·∫ßu v√†o:
        - X: Input tensor (batch_size, in_channels, H, W)
        - is_inference: Boolean, ch·∫ø ƒë·ªô inference hay training
        
        ƒê·∫ßu ra:
        - out: Output tensor (batch_size, out_channels, H', W')
        """
        residual = X  # L∆∞u identity connection
        
        # Main path: Conv1x1 -> BN -> ReLU -> Conv3x3 -> BN
        out = self.conv1.forward(X)
        out = self.bn1.forward(out)
        out = self.relu1.forward(out)
        out = self.conv2.forward(out)
        out = self.bn2.forward(out)
        
        # Shortcut connection v·ªõi downsample n·∫øu c·∫ßn
        if self.downsample:
            residual = self.downsample.forward(X)
            residual = self.bn_down.forward(residual)
        
        # Element-wise addition: F(x) + x
        out += residual
        
        # Final ReLU activation
        out = self.relu_final.forward(out)
        return out

    def backward(self, dout, is_inference=False):
        dout = self.relu_final.backward(dout)
        dout_res = dout
        dout = self.bn2.backward(dout)
        dout = self.conv2.backward(dout)
        dout = self.relu1.backward(dout)
        dout = self.bn1.backward(dout)
        dout = self.conv1.backward(dout)
        if self.downsample:
            dresidual = self.bn_down.backward(self.downsample.backward(dout_res))
            dout += self.downsample.backward(dresidual)
        else:
            dout += dout_res
        return dout

class ResNet50:
    """
    ResNet-50 Architecture Implementation
    ====================================
    
    Ki·∫øn tr√∫c chi ti·∫øt:
    1. Conv1: 7x7, 64 filters, stride=1
    2. MaxPool: 3x3, stride=1
    3. Layer1: 3 residual blocks, 64 filters
    4. Layer2: 4 residual blocks, 128 filters, stride=2 (downsample)
    5. Layer3: 6 residual blocks, 256 filters, stride=2 (downsample)
    6. Layer4: 3 residual blocks, 512 filters, stride=2 (downsample)
    7. Global Average Pooling
    8. Fully Connected: 512 -> num_classes
    9. Softmax activation
    
    T·ªïng s·ªë layers: 1 + 2*(3+4+6+3) + 1 = 34 layers (conv + fc)
    
    ƒê·∫ßu v√†o:
    - X: Images tensor (batch_size, channels, height, width)
    - y: Labels tensor (batch_size,) ho·∫∑c (batch_size, num_classes)
    
    ƒê·∫ßu ra:
    - Logits/probabilities: (batch_size, num_classes)
    """
    def __init__(self, num_classes=NUM_CLASSES):
        self.num_classes = num_classes
        self.conv1 = Conv(64, 7, INPUT_SHAPE[0], stride=1, padding=3)
        self.bn1 = BatchNorm(64)
        self.relu1 = ReLU()
        self.maxpool = MaxPool(3, stride=1, padding=1)  # Keep size ~24
        
        # Residual layers (scale down s·ªë blocks cho input nh·ªè)
        # Layer 1: 3 blocks, 64 filters, no downsample
        self.layer1 = [ResidualBlock(64, 64, stride=1, downsample=False) for _ in range(3)]

        # Layer 2: 4 blocks, 128 filters, first block downsamples
        self.layer2 = [ResidualBlock(64, 128, stride=2, downsample=True)] + \
                    [ResidualBlock(128, 128, stride=1, downsample=False) for _ in range(3)]

        # Layer 3: 6 blocks, 256 filters, first block downsamples
        self.layer3 = [ResidualBlock(128, 256, stride=2, downsample=True)] + \
                    [ResidualBlock(256, 256, stride=1, downsample=False) for _ in range(5)]

        # Layer 4: 3 blocks, 512 filters, first block downsamples
        self.layer4 = [ResidualBlock(256, 512, stride=2, downsample=True)] + \
                    [ResidualBlock(512, 512, stride=1, downsample=False) for _ in range(2)]
        
        # Classifier
        self.avgpool = None  # S·∫Ω d√πng np.mean trong forward
        self.fc = Fc(512 * 1 * 1, num_classes)  # Gi·∫£ ƒë·ªãnh sau layer4 l√† 1x1
        self.softmax = Softmax()
        self.loss_fn = CrossEntropyLoss()
        
        # All layers ƒë·ªÉ collect params (kh√¥ng bao g·ªìm softmax v√¨ kh√¥ng c√≥ params)
        self.all_layers = [self.conv1, self.bn1, self.relu1, self.maxpool] + \
                          sum([self.layer1, self.layer2, self.layer3, self.layer4], []) + \
                          [self.fc]
        
        self.params = self._collect_params()
        self.optimizer = Adam(lr=0.001)
        self.is_inference = False

    def _collect_params(self):
        params = {}
        for layer in self.all_layers:
            if isinstance(layer, (Conv, Fc)):
                params[f'W_{id(layer)}'] = layer.W['val']
                params[f'b_{id(layer)}'] = layer.b['val']
            elif isinstance(layer, BatchNorm):
                params[f'gamma_{id(layer)}'] = layer.gamma['val']
                params[f'beta_{id(layer)}'] = layer.beta['val']
            elif isinstance(layer, ResidualBlock):
                # Residual blocks contain Conv and BN layers
                if hasattr(layer, 'conv1') and isinstance(layer.conv1, Conv):
                    params[f'W_{id(layer.conv1)}'] = layer.conv1.W['val']
                    params[f'b_{id(layer.conv1)}'] = layer.conv1.b['val']
                if hasattr(layer, 'conv2') and isinstance(layer.conv2, Conv):
                    params[f'W_{id(layer.conv2)}'] = layer.conv2.W['val']
                    params[f'b_{id(layer.conv2)}'] = layer.conv2.b['val']
                if hasattr(layer, 'bn1') and isinstance(layer.bn1, BatchNorm):
                    params[f'gamma_{id(layer.bn1)}'] = layer.bn1.gamma['val']
                    params[f'beta_{id(layer.bn1)}'] = layer.bn1.beta['val']
                if hasattr(layer, 'bn2') and isinstance(layer.bn2, BatchNorm):
                    params[f'gamma_{id(layer.bn2)}'] = layer.bn2.gamma['val']
                    params[f'beta_{id(layer.bn2)}'] = layer.bn2.beta['val']
                if hasattr(layer, 'downsample') and layer.downsample is not None:
                    if hasattr(layer.downsample, 'conv') and isinstance(layer.downsample.conv, Conv):
                        params[f'W_{id(layer.downsample.conv)}'] = layer.downsample.conv.W['val']
                        params[f'b_{id(layer.downsample.conv)}'] = layer.downsample.conv.b['val']
        return params

    def forward(self, X):
        """
        Forward pass qua to√†n b·ªô network
        
        ƒê·∫ßu v√†o:
        - X: Input images (batch_size, channels, height, width)
        
        ƒê·∫ßu ra:
        - y_pred: Predicted probabilities (batch_size, num_classes)
        """
        out = X
        
        # Conv1 + BN + ReLU + MaxPool
        out = self.conv1.forward(out)
        out = self.bn1.forward(out)
        out = self.relu1.forward(out)
        out = self.maxpool.forward(out)
        
        # Residual blocks
        for block in self.layer1:
            out = block.forward(out, self.is_inference)
        for block in self.layer2:
            out = block.forward(out, self.is_inference)
        for block in self.layer3:
            out = block.forward(out, self.is_inference)
        for block in self.layer4:
            out = block.forward(out, self.is_inference)
        
        # Global Average Pooling
        out = np.mean(out, axis=(2, 3))  # (batch_size, channels)
        
        # Fully Connected
        logits = self.fc.forward(out)
        
        # Softmax
        y_pred = self.softmax.forward(logits)
        
        return y_pred

    def backward(self, y_pred, y):
        """
        Backward pass ƒë·ªÉ t√≠nh gradients
        
        ƒê·∫ßu v√†o:
        - y_pred: Predicted probabilities
        - y: True labels (one-hot)
        
        L∆∞u √Ω: Ch·ªâ train FC layer, freeze feature extractor ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a
        """
        # Softmax + CrossEntropy backward
        dout = self.softmax.backward(y_pred, y)
        
        # FC backward - ch·ªâ c·∫ßn t√≠nh gradient cho FC layer
        dout = self.fc.backward(dout)
        
        # Kh√¥ng backprop qua c√°c layer kh√°c (frozen features)
        # N·∫øu mu·ªën train full network, c·∫ßn implement global avg pool backward
        # v√† backprop qua t·∫•t c·∫£ residual blocks

    def train_step(self, X, y):
        """
        Training step with forward + backward
        
        Accepts both integer labels and one-hot encoded labels.
        """
        # N·∫øu y ch∆∞a ph·∫£i one-hot (shape = (batch_size,)), th√¨ one-hot encode
        if len(y.shape) == 1:
            y_onehot = np.eye(self.num_classes)[y]
        else:
            # N·∫øu y ƒë√£ l√† one-hot (shape = (batch_size, num_classes)), d√πng lu√¥n
            y_onehot = y
        
        # Set training mode
        self.set_inference(False)
        
        # Forward pass
        y_pred = self.forward(X)
        
        # Verify shapes for debugging (only print once)
        if not hasattr(self, '_debug_printed'):
            self._debug_printed = True
            # Only print if shapes look unexpected
            if y_pred.shape != y_onehot.shape:
                print(f"‚ö†Ô∏è  Shape mismatch in train_step: y_pred {y_pred.shape} vs y_onehot {y_onehot.shape}")
        
        # Loss computation
        loss = self.loss_fn.get(y_pred, y_onehot)
        
        # Backward pass
        self.backward(y_pred, y_onehot)
        
        # Collect gradients
        grads = self._collect_grads()
        
        # Debug: Check gradients on first call
        if not hasattr(self, '_grad_check_done'):
            self._grad_check_done = True
            num_grads = len(grads)
            num_nonzero = sum(1 for g in grads.values() if g is not None and np.sum(np.abs(g)) > 0)
            print(f"\nüîç Gradient check:")
            print(f"   Total parameters: {len(self.params)}")
            print(f"   Total gradients: {num_grads}")
            print(f"   Non-zero gradients: {num_nonzero}")
            if num_nonzero == 0:
                print(f"   ‚ö†Ô∏è  WARNING: All gradients are zero! Backprop may not be working.")
            # Sample one gradient
            sample_key = list(grads.keys())[0]
            sample_grad = grads[sample_key]
            print(f"   Sample gradient '{sample_key[:20]}...': mean={np.mean(np.abs(sample_grad)):.6f}, max={np.max(np.abs(sample_grad)):.6f}\n")
        
        # Update parameters
        self.optimizer.update(self.params, grads)
        
        return loss
    
    def predict(self, X):
        self.is_inference = True
        y_pred = self.forward(X)
        self.is_inference = False
        return np.argmax(y_pred, axis=1)

    def set_inference(self, mode=True):
        self.is_inference = mode
        for layer in self.all_layers:
            if isinstance(layer, BatchNorm):
                layer.use_moving_avg = mode
    def _collect_grads(self):
        grads = {}
        for layer in self.all_layers:
            if isinstance(layer, (Conv, Fc)):
                grads[f'W_{id(layer)}'] = layer.W['grad']
                grads[f'b_{id(layer)}'] = layer.b['grad']
            elif isinstance(layer, BatchNorm):
                grads[f'gamma_{id(layer)}'] = layer.gamma['grad']
                grads[f'beta_{id(layer)}'] = layer.beta['grad']
            elif isinstance(layer, ResidualBlock):
                # Residual blocks contain Conv and BN layers
                if hasattr(layer, 'conv1') and isinstance(layer.conv1, Conv):
                    grads[f'W_{id(layer.conv1)}'] = layer.conv1.W['grad']
                    grads[f'b_{id(layer.conv1)}'] = layer.conv1.b['grad']
                if hasattr(layer, 'conv2') and isinstance(layer.conv2, Conv):
                    grads[f'W_{id(layer.conv2)}'] = layer.conv2.W['grad']
                    grads[f'b_{id(layer.conv2)}'] = layer.conv2.b['grad']
                if hasattr(layer, 'bn1') and isinstance(layer.bn1, BatchNorm):
                    grads[f'gamma_{id(layer.bn1)}'] = layer.bn1.gamma['grad']
                    grads[f'beta_{id(layer.bn1)}'] = layer.bn1.beta['grad']
                if hasattr(layer, 'bn2') and isinstance(layer.bn2, BatchNorm):
                    grads[f'gamma_{id(layer.bn2)}'] = layer.bn2.gamma['grad']
                    grads[f'beta_{id(layer.bn2)}'] = layer.bn2.beta['grad']
                if hasattr(layer, 'downsample') and layer.downsample is not None:
                    if hasattr(layer.downsample, 'conv') and isinstance(layer.downsample.conv, Conv):
                        grads[f'W_{id(layer.downsample.conv)}'] = layer.downsample.conv.W['grad']
                        grads[f'b_{id(layer.downsample.conv)}'] = layer.downsample.conv.b['grad']
        return grads