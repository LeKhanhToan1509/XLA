"""
Training Module for ResNet50
============================

Qu√° tr√¨nh Training:
1. Forward pass: X ‚Üí ResNet50 ‚Üí predictions
2. Loss calculation: CrossEntropy(predictions, true_labels) 
3. Backward pass: Backpropagation ƒë·ªÉ t√≠nh gradients
4. Parameter update: Adam optimizer c·∫≠p nh·∫≠t weights
5. Validation: ƒê√°nh gi√° model tr√™n validation set

C√¥ng th·ª©c Training Loop:
For each epoch:
  For each batch:
    1. y_pred = model.forward(X_batch)
    2. loss = CrossEntropy(y_pred, y_true)
    3. gradients = model.backward(loss)
    4. optimizer.update(parameters, gradients)

ƒê·∫ßu v√†o:
- X_train: Training images (N_train, channels, height, width)
- y_train: Training labels (N_train,)
- X_val: Validation images (N_val, channels, height, width) 
- y_val: Validation labels (N_val,)
- num_classes: S·ªë l∆∞·ª£ng classes ƒë·ªÉ classify

ƒê·∫ßu ra:
- model: Trained ResNet50 model
"""

try:
    import cupy as np
    print("‚úÖ Using GPU (CuPy)")
    GPU_AVAILABLE = True
except ImportError:
    import numpy as np
    print("‚ö†Ô∏è  Using CPU (NumPy)")
    GPU_AVAILABLE = False
from ai.model.resnet import ResNet50
from ai.data.dataloader import DataLoader
from ai.configs.config import EPOCHS, LEARNING_RATE
from ai.model.layers import CrossEntropyLoss
from tqdm import tqdm
import time

def train_model(X_train, y_train, X_val, y_val, num_classes=10):  # ƒêi·ªÅu ch·ªânh num_classes
    from ai.configs.config import BATCH_SIZE
    
    model = ResNet50(num_classes=num_classes)
    train_loader = DataLoader(X_train, y_train, BATCH_SIZE)
    val_loader = DataLoader(X_val, y_val, BATCH_SIZE, shuffle=False)
    
    loss_fn = CrossEntropyLoss()
    optimizer = model.optimizer  # ƒê√£ init trong model
    
    print(f"‚úÖ Model initialized!\n")
    
    best_val_acc = 0.0
    
    for epoch in range(EPOCHS):
        # Clear GPU cache every few epochs to prevent OOM on 4GB VRAM
        if GPU_AVAILABLE and epoch > 0 and epoch % 3 == 0:
            try:
                import cupy as cp
                cp.get_default_memory_pool().free_all_blocks()
                print(f"\nüßπ Cleared GPU memory cache")
            except:
                pass
        
        print(f"\n{'='*60}")
        print(f"üìä Epoch {epoch+1}/{EPOCHS}")
        print(f"{'='*60}")
        
        train_loader.reset()
        total_loss = 0
        batch_count = 0
        
        # Training loop v·ªõi progress bar
        num_batches = len(X_train) // BATCH_SIZE + (1 if len(X_train) % BATCH_SIZE != 0 else 0)
        pbar = tqdm(train_loader, total=num_batches, desc=f"Training", 
                    bar_format='{l_bar}{bar:30}{r_bar}')
        
        start_time = time.time()
        
        for X_batch, y_batch in pbar:
            y_onehot = train_loader.one_hot(y_batch, num_classes)
            
            # Debug log for first batch of first epoch to verify shapes
            if epoch == 0 and batch_count == 0:
                print(f"\nüîç Debug - First batch shapes:")
                print(f"   X_batch: {X_batch.shape}")
                print(f"   y_batch: {y_batch.shape} (integer labels)")
                print(f"   y_onehot: {y_onehot.shape} (one-hot encoded)")
                print(f"   y_batch sample: {y_batch[:5]}")
                print(f"   y_onehot sample:\n{y_onehot[:2]}\n")
            
            loss = model.train_step(X_batch, y_onehot)
            total_loss += loss
            batch_count += 1
            
            # Update progress bar
            pbar.set_postfix({'loss': f'{loss:.4f}', 'avg_loss': f'{total_loss/batch_count:.4f}'})
        
        epoch_time = time.time() - start_time
        
        # Guard against division by zero when no batches were processed
        if batch_count == 0:
            print(f"\n‚ö†Ô∏è  Warning: No training batches were processed in epoch {epoch+1}!")
            print(f"   Please check if training data is empty or batch size is too large.")
            continue
        
        avg_loss = total_loss / batch_count
        
        print(f"\nüìâ Training Results:")
        print(f"   - Average Loss: {avg_loss:.4f}")
        print(f"   - Time: {epoch_time:.2f}s ({epoch_time/60:.2f}m)")
        
        # Validation (skip if validation set is empty)
        if len(X_val) > 0:
            print(f"\nüîç Validating...")
            val_acc = evaluate(model, val_loader, num_classes)
            print(f"‚úÖ Validation Accuracy: {val_acc*100:.2f}%")
            
            # Track best model
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                print(f"üéØ New best validation accuracy!")
        else:
            print(f"\n‚ö†Ô∏è  Skipping validation (no validation data)")
    
    print(f"\n{'='*60}")
    print(f"üèÜ Training completed!")
    print(f"   - Best validation accuracy: {best_val_acc*100:.2f}%")
    print(f"{'='*60}")
    
    return model

def evaluate(model, loader, num_classes):
    """
    Evaluation Function
    ==================
    
    ƒê√°nh gi√° accuracy c·ªßa model tr√™n validation/test set
    
    C√¥ng th·ª©c Accuracy:
    Accuracy = S·ªë predictions ƒë√∫ng / T·ªïng s·ªë samples
             = Œ£(predicted_class == true_class) / N
    
    ƒê·∫ßu v√†o:
    - model: Trained model c·∫ßn evaluate
    - loader: DataLoader ch·ª©a evaluation data
    - num_classes: S·ªë classes (kh√¥ng d√πng trong h√†m n√†y)
    
    ƒê·∫ßu ra:
    - accuracy: Float value trong kho·∫£ng [0, 1]
    
    L∆∞u √Ω:
    - Set model v·ªÅ inference mode (BatchNorm d√πng moving stats)
    - Kh√¥ng c·∫ßn gradients trong evaluation
    """
    correct = 0
    total = 0
    
    # Chuy·ªÉn sang inference mode
    model.set_inference(True)
    
    for X_batch, y_batch in loader:
        # Forward pass ƒë·ªÉ l·∫•y predictions
        y_pred = model.predict(X_batch)  # Returns predicted class indices
        
        # ƒê·∫øm s·ªë predictions ƒë√∫ng
        correct += np.sum(y_pred == y_batch)
        total += len(y_batch)
    
    # Chuy·ªÉn v·ªÅ training mode
    model.set_inference(False)

    # T√≠nh accuracy: guard against zero total to avoid ZeroDivisionError
    if total == 0:
        print("\n\u26a0\ufe0f  Warning: No samples were provided to evaluate() (total=0). Returning accuracy=0.0")
        return 0.0

    return correct / total
